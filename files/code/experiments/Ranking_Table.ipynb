{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af58ee8",
   "metadata": {},
   "source": [
    "### **Cell 1: Imports and Initial Setup**\n",
    "\n",
    "This cell imports the necessary libraries and sets up basic configurations like display options and warning filters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be65847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from IPython.display import HTML, display\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Configure pandas to display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116a2db",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 2: Configuration of Algorithms and Datasets**\n",
    "\n",
    "This cell defines the paths to the algorithm result files and lists the datasets to be analyzed. This centralized configuration makes it easy to manage inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping algorithm names to their result file paths\n",
    "algorithms = {\n",
    "    'AVF': r'..\\..\\results\\experiments\\algorithms\\AVF\\AVF.csv',\n",
    "    'CBRW': r'..\\..\\results\\experiments\\algorithms\\CBRW\\CBRW.csv',\n",
    "    'CompreX': r'..\\..\\results\\experiments\\algorithms\\CompreX\\COMPREX.csv',\n",
    "    'FPOF': r'..\\..\\results\\experiments\\algorithms\\FPOF\\FPOF.csv',\n",
    "    'POP': r'..\\..\\results\\experiments\\algorithms\\POP\\POP.csv',\n",
    "    'SCAN': r'..\\..\\results\\experiments\\algorithms\\SCAN\\SCAN.csv',\n",
    "    'SDRW': r'..\\..\\results\\experiments\\algorithms\\SDRW\\SDRW.csv',\n",
    "    'Zero++': r'..\\..\\results\\experiments\\algorithms\\Zero++\\ZERO++.csv',\n",
    "    \n",
    "    'DeepSVDD_ca': r'..\\..\\results\\experiments\\algorithms\\DeepSVDD\\ca\\CA.csv',\n",
    "    'DeepSVDD_idf': r'..\\..\\results\\experiments\\algorithms\\DeepSVDD\\idf\\IDF.csv',\n",
    "    'DeepSVDD_one': r'..\\..\\results\\experiments\\algorithms\\DeepSVDD\\one_hot\\ONE_HOT.csv',\n",
    "    'DeepSVDD_pivot': r'..\\..\\results\\experiments\\algorithms\\DeepSVDD\\pivot\\PIVOT.csv',\n",
    "    'DeepSVDD_nocat': r'..\\..\\results\\experiments\\algorithms\\DeepSVDD\\nocat\\NOCAT.csv',\n",
    "    'FastABOD_ca': r'..\\..\\results\\experiments\\algorithms\\FastABOD\\ca\\CA.csv',\n",
    "    'FastABOD_idf': r'..\\..\\results\\experiments\\algorithms\\FastABOD\\idf\\IDF.csv',\n",
    "    'FastABOD_one': r'..\\..\\results\\experiments\\algorithms\\FastABOD\\one_hot\\ONE_HOT.csv',\n",
    "    'FastABOD_pivot': r'..\\..\\results\\experiments\\algorithms\\FastABOD\\pivot\\PIVOT.csv',\n",
    "    'FastABOD_nocat': r'..\\..\\results\\experiments\\algorithms\\FastABOD\\nocat\\NOCAT.csv',\n",
    "    'iForest_ca': r'..\\..\\results\\experiments\\algorithms\\iForest\\ca\\CA.csv',\n",
    "    'iForest_idf': r'..\\..\\results\\experiments\\algorithms\\iForest\\idf\\IDF.csv',\n",
    "    'iForest_one': r'..\\..\\results\\experiments\\algorithms\\iForest\\one_hot\\ONE_HOT.csv',\n",
    "    'iForest_pivot': r'..\\..\\results\\experiments\\algorithms\\iForest\\pivot\\PIVOT.csv',\n",
    "    'iForest_nocat': r'..\\..\\results\\experiments\\algorithms\\iForest\\nocat\\NOCAT.csv',\n",
    "    'KNN_ca': r'..\\..\\results\\experiments\\algorithms\\KNN\\ca\\CA.csv',\n",
    "    'KNN_idf': r'..\\..\\results\\experiments\\algorithms\\KNN\\idf\\IDF.csv',\n",
    "    'KNN_one': r'..\\..\\results\\experiments\\algorithms\\KNN\\one_hot\\ONE_HOT.csv',\n",
    "    'KNN_pivot': r'..\\..\\results\\experiments\\algorithms\\KNN\\pivot\\PIVOT.csv',\n",
    "    'KNN_nocat': r'..\\..\\results\\experiments\\algorithms\\KNN\\nocat\\NOCAT.csv',\n",
    "    'LOF_ca': r'..\\..\\results\\experiments\\algorithms\\LOF\\ca\\CA.csv',\n",
    "    'LOF_idf': r'..\\..\\results\\experiments\\algorithms\\LOF\\idf\\IDF.csv',\n",
    "    'LOF_one': r'..\\..\\results\\experiments\\algorithms\\LOF\\one_hot\\ONE_HOT.csv',\n",
    "    'LOF_pivot': r'..\\..\\results\\experiments\\algorithms\\LOF\\pivot\\PIVOT.csv',\n",
    "    'LOF_nocat': r'..\\..\\results\\experiments\\algorithms\\LOF\\nocat\\NOCAT.csv',\n",
    "    'McCatch_ca': r'..\\..\\results\\experiments\\algorithms\\McCatch\\ca\\CA.csv',\n",
    "    'McCatch_idf': r'..\\..\\results\\experiments\\algorithms\\McCatch\\idf\\IDF.csv',\n",
    "    'McCatch_one': r'..\\..\\results\\experiments\\algorithms\\McCatch\\one_hot\\ONE_HOT.csv',\n",
    "    'McCatch_pivot': r'..\\..\\results\\experiments\\algorithms\\McCatch\\pivot\\PIVOT.csv',\n",
    "    'McCatch_nocat': r'..\\..\\results\\experiments\\algorithms\\McCatch\\nocat\\NOCAT.csv',\n",
    " }\n",
    "\n",
    "# List of all datasets included in the study\n",
    "all_dataset_names = [\n",
    "    'ad_nominal', 'AID362red_train_allpossiblenominal', 'apascal_entire_trainvsall',\n",
    "    'cmc-nominal', 'covertype_nominal_4vs123567', 'kddcup99-corrected-u2rvsnormal-nominal-cleaned',\n",
    "    'list_attr_celeba_baldvsnonbald', 'Reuters-corn-100', 'w7a-libsvm-nonsparse',\n",
    "    'bank-additional-ful-nominal', 'solar-flare_FvsAll-cleaned', 'anneal',\n",
    "    'australian', 'crx', 'ecoli', 'german', 'heart', 'hepatitis', 'lymphography',\n",
    "    'nursery', 'bands_band_16_variant1ori', 'creditA_plus_42_variant1ori',\n",
    "    'sick_sick_35_variant1ori', 'thyroid_disease_variant1ori', 'scenario-1-1-rcat-0-icat',\n",
    "    'scenario-1-2-rcat-0-icat', 'scenario-1-3-rcat-0-icat', 'scenario-1-4-rcat-0-icat',\n",
    "    'scenario-1-5-rcat-0-icat', 'scenario-1-6-rcat-0-icat', 'scenario-1-7-rcat-0-icat',\n",
    "    'scenario-1-8-rcat-0-icat', 'scenario-1-9-rcat-0-icat', 'scenario-1-9-rcat-1-icat',\n",
    "    'scenario-1-9-rcat-2-icat', 'scenario-1-9-rcat-3-icat', 'scenario-1-9-rcat-4-icat',\n",
    "    'scenario-1-9-rcat-5-icat', 'scenario-1-9-rcat-6-icat', 'scenario-1-9-rcat-7-icat',\n",
    "    'scenario-1-9-rcat-8-icat', 'scenario-1-9-rcat-9-icat', 'KDDTrain20FS',\n",
    "    'KDDTrain20ProbeFS', 'KDDTrain20R2LFS', 'census-income-full-nominal',\n",
    "    'chess_krkopt_zerovsall'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a035b3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 3: Helper Functions for Ranking and Styling**\n",
    "\n",
    "This cell contains utility functions for calculating ranks and for styling DataFrames by highlighting the best value in each row.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadeb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_values(value_list: list) -> list:\n",
    "    \"\"\"\n",
    "    Calculates ranks for a list of values, with 1 being the best (highest value).\n",
    "    Handles ties by assigning them the same rank.\n",
    "    \"\"\"\n",
    "    series = pd.Series(value_list)\n",
    "    # Rank in descending order, assigning the minimum rank in case of a tie\n",
    "    return series.rank(ascending=False, method='min').tolist()\n",
    "\n",
    "def highlight_min_in_row(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Applies a style to a DataFrame to bold the minimum value in each row.\n",
    "    This is used to highlight the best rank (rank 1).\n",
    "    \"\"\"\n",
    "    def style_min(row):\n",
    "        # Clean row data, handling strings with '±' for mean/std deviation\n",
    "        try:\n",
    "            numeric_row = row.apply(lambda x: float(str(x).split('\\u00B1')[0]))\n",
    "        except (ValueError, TypeError):\n",
    "            return ['' for _ in row] # Return no style if conversion fails\n",
    "            \n",
    "        is_min = numeric_row == numeric_row.min()\n",
    "        return ['font-weight: bold' if v else '' for v in is_min]\n",
    "    \n",
    "    return df.style.apply(style_min, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad9c74e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 4: Core Data Loading and Processing Function**\n",
    "\n",
    "This is the main function responsible for loading raw result files, processing them based on the desired metric and aggregation type (`best` or `average`), calculating ranks, and saving the final table to a CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c46987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ranking_table(allowed_datasets: list, output_filename: str, metric: str = 'auc', agg_type: str = 'average'):\n",
    "    \"\"\"\n",
    "    Loads, processes, and ranks algorithm results for a specific group of datasets.\n",
    "    \n",
    "    Args:\n",
    "        allowed_datasets (list): A list of dataset names to include in the table.\n",
    "        output_filename (str): The name of the CSV file to save the results.\n",
    "        metric (str): The performance metric to use (e.g., 'auc').\n",
    "        agg_type (str): The aggregation type ('average' or 'best').\n",
    "    \"\"\"\n",
    "    # Default values for missing data\n",
    "    default_values = {'auc': 0.5, 'adj_r_precision': 0, 'adj_average_precision': 0, 'adj_max_f1': 0}\n",
    "    \n",
    "    # --- 1. Load and Aggregate Results ---\n",
    "    results_data = {}\n",
    "    allowed_datasets_clean = [d.replace('.csv', '') for d in allowed_datasets]\n",
    "\n",
    "    for algo_name, path in algorithms.items():\n",
    "        processed_rows = []\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=';').drop_duplicates()\n",
    "            df['dataset'] = df['dataset'].str.replace(r'(_v[0-9]{1,2})?\\.csv$', '', regex=True)\n",
    "        except FileNotFoundError:\n",
    "            df = pd.DataFrame() # Create empty DataFrame if file not found\n",
    "\n",
    "        for dataset_name in allowed_datasets_clean:\n",
    "            df_subset = df[df['dataset'] == dataset_name]\n",
    "            \n",
    "            if df_subset.empty:\n",
    "                # Add default value if dataset is missing for this algorithm\n",
    "                processed_rows.append(pd.Series([dataset_name, default_values[metric]], index=['dataset', metric]))\n",
    "                continue\n",
    "\n",
    "            if agg_type == 'average':\n",
    "                # Calculate the mean score across all parameters\n",
    "                mean_score = df_subset[metric].mean()\n",
    "                processed_rows.append(pd.Series([dataset_name, mean_score], index=['dataset', metric]))\n",
    "            else: # 'best'\n",
    "                # Find the highest score among all parameters\n",
    "                best_row = df_subset.loc[df_subset[metric].idxmax()]\n",
    "                processed_rows.append(best_row)\n",
    "        \n",
    "        results_data[algo_name] = pd.DataFrame(processed_rows)[['dataset', metric]].reset_index(drop=True)\n",
    "\n",
    "    # --- 2. Assemble Performance and Ranking Tables ---\n",
    "    columns = ['Dataset'] + list(results_data.keys())\n",
    "    performance_rows = []\n",
    "    ranking_rows = []\n",
    "\n",
    "    for dataset_name in allowed_datasets_clean:\n",
    "        perf_row = [dataset_name]\n",
    "        for algo_name, df_result in results_data.items():\n",
    "            score = df_result.query(\"dataset == @dataset_name\")[metric].iloc[0]\n",
    "            perf_row.append(score)\n",
    "        \n",
    "        performance_rows.append(perf_row)\n",
    "        # Calculate ranks for the current dataset's performance row\n",
    "        ranking_rows.append([dataset_name] + rank_values(perf_row[1:]))\n",
    "\n",
    "    df_performance = pd.DataFrame(performance_rows, columns=columns)\n",
    "    df_ranking = pd.DataFrame(ranking_rows, columns=columns)\n",
    "\n",
    "    # --- 3. Calculate and Append Summary Statistics ---\n",
    "    avg_ranks = []\n",
    "    avg_metrics = []\n",
    "    for col in df_performance.columns[1:]:\n",
    "        avg_ranks.append(f\"{df_ranking[col].mean():.2f} ± {df_ranking[col].std():.2f}\")\n",
    "        avg_metrics.append(f\"{df_performance[col].mean():.3f} ± {df_performance[col].std():.2f}\")\n",
    "    \n",
    "    # Append summary rows to the performance table\n",
    "    df_performance.loc[len(df_performance)] = [f'Average {metric.upper()}'] + avg_metrics\n",
    "    df_performance.loc[len(df_performance)] = ['Average Rank'] + avg_ranks\n",
    "    \n",
    "    # --- 4. Save the Final Table ---\n",
    "    output_dir = os.path.join(r'..\\..\\results\\experiments\\tables', agg_type)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df_performance.to_csv(os.path.join(output_dir, f'{output_filename}_{metric}.csv'), index=False, sep=';')\n",
    "\n",
    "    return df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7d11a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 5: Dataset Categorization Functions**\n",
    "\n",
    "This block defines functions to group datasets based on their properties (e.g., number of features, number of instances, data type). This allows for more granular analysis of algorithm performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset summary file\n",
    "df_summary = pd.read_csv(r'..\\resume_datasets.csv', sep=';')\n",
    "\n",
    "def get_datasets_by_feature_count(low_thresh, med_thresh):\n",
    "    \"\"\"Groups datasets by the number of features.\"\"\"\n",
    "    low = df_summary[df_summary['features'] <= low_thresh]['file'].tolist()\n",
    "    medium = df_summary[(df_summary['features'] > low_thresh) & (df_summary['features'] <= med_thresh)]['file'].tolist()\n",
    "    high = df_summary[df_summary['features'] > med_thresh]['file'].tolist()\n",
    "    return {'low_feature': low, 'medium_feature': medium, 'high_feature': high}\n",
    "\n",
    "def get_datasets_by_instance_count(low_thresh, med_thresh):\n",
    "    \"\"\"Groups datasets by the number of instances.\"\"\"\n",
    "    low = df_summary[df_summary['instances'] <= low_thresh]['file'].tolist()\n",
    "    medium = df_summary[(df_summary['instances'] > low_thresh) & (df_summary['instances'] <= med_thresh)]['file'].tolist()\n",
    "    high = df_summary[df_summary['instances'] > med_thresh]['file'].tolist()\n",
    "    return {'low_instance': low, 'medium_instance': medium, 'high_instance': high}\n",
    "\n",
    "def get_datasets_by_categorical_percentage(low_thresh, med_thresh):\n",
    "    \"\"\"Groups mixed-type datasets by the percentage of categorical features.\"\"\"\n",
    "    df_mixed = df_summary[df_summary['attr_categorics'] > 0]\n",
    "    low = df_mixed[df_mixed['%_categorics'] <= low_thresh]['file'].tolist()\n",
    "    medium = df_mixed[(df_mixed['%_categorics'] > low_thresh) & (df_mixed['%_categorics'] <= med_thresh)]['file'].tolist()\n",
    "    high = df_mixed[df_mixed['%_categorics'] > med_thresh]['file'].tolist()\n",
    "    return {'low_categorical': low, 'medium_categorical': medium, 'high_categorical': high}\n",
    "\n",
    "def get_datasets_by_binary_percentage(threshold):\n",
    "    \"\"\"For purely categorical datasets, splits them based on the percentage of binary features.\"\"\"\n",
    "    df_cat_only = df_summary[df_summary['%_categorics'] >= 100]\n",
    "    low = df_cat_only[df_cat_only['%_binaries'] <= threshold]['file'].tolist()\n",
    "    high = df_cat_only[df_cat_only['%_binaries'] > threshold]['file'].tolist()\n",
    "    return {'low_binary': low, 'high_binary': high}\n",
    "\n",
    "def get_datasets_by_context():\n",
    "    \"\"\"Groups datasets by their context/domain.\"\"\"\n",
    "    return {\n",
    "        'medicine_context': ['thyroid_disease_variant1ori.csv', 'sick_sick_35_variant1ori.csv', 'cmc-nominal.csv', 'ecoli.csv', 'heart.csv', 'hepatitis.csv', 'lymphography.csv', 'nursery.csv'],\n",
    "        'finance_context': ['bank-additional-ful-nominal_processed.csv', 'creditA_plus_42_variant1ori.csv', 'australian.csv', 'crx.csv', 'german.csv', 'Reuters-corn-100.csv'],\n",
    "        'invasion_context': ['kddcup99-corrected-u2rvsnormal-nominal-cleaned.csv', 'KDDTrain20R2LFS.csv', 'KDDTrain20ProbeFS.csv', 'KDDTrain20FS.csv'],\n",
    "        'sciency_context': ['solar-flare_FvsAll-cleaned_processed.csv', 'bands_band_16_variant1ori.csv', 'anneal.csv', 'covertype_nominal_4vs123567.csv', 'AID362red_train_allpossiblenominal.csv', 'list_attr_celeba_baldvsnonbald.csv', 'w7a-libsvm-nonsparse.csv'],\n",
    "        'synthetic_context': ['scenario-1-1-rcat-0-icat.csv', 'scenario-1-2-rcat-0-icat.csv', 'scenario-1-3-rcat-0-icat.csv', 'scenario-1-4-rcat-0-icat.csv', 'scenario-1-5-rcat-0-icat.csv', 'scenario-1-6-rcat-0-icat.csv',\n",
    "                          'scenario-1-7-rcat-0-icat.csv', 'scenario-1-8-rcat-0-icat.csv', 'scenario-1-9-rcat-0-icat.csv', 'scenario-1-9-rcat-1-icat.csv', 'scenario-1-9-rcat-2-icat.csv', 'scenario-1-9-rcat-3-icat.csv',\n",
    "                          'scenario-1-9-rcat-4-icat.csv', 'scenario-1-9-rcat-5-icat.csv', 'scenario-1-9-rcat-6-icat.csv', 'scenario-1-9-rcat-7-icat.csv', 'scenario-1-9-rcat-8-icat.csv', 'scenario-1-9-rcat-9-icat.csv']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778d475",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 6: Main Execution: Generate All Ranking Tables**\n",
    "\n",
    "This is the main execution block. It iterates through all metrics, aggregation types, and dataset groups to generate and save all the ranking tables for the entire study.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f816a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_process = ['auc', 'adj_r_precision', 'adj_average_precision', 'adj_max_f1']\n",
    "aggregation_types = ['average', 'best']\n",
    "\n",
    "# Define all dataset groupings\n",
    "dataset_groups = {\n",
    "    'general': all_dataset_names,\n",
    "    **get_datasets_by_feature_count(15, 25),\n",
    "    **get_datasets_by_instance_count(10000, 20000),\n",
    "    **get_datasets_by_categorical_percentage(34, 67),\n",
    "    **get_datasets_by_binary_percentage(50),\n",
    "    **get_datasets_by_context(),\n",
    "}\n",
    "\n",
    "# Main loop to generate all tables\n",
    "for agg_type in aggregation_types:\n",
    "    for metric in metrics_to_process:\n",
    "        for group_name, dataset_list in dataset_groups.items():\n",
    "            if not dataset_list:\n",
    "                print(f\"Skipping {group_name} for {metric} ({agg_type}) - No datasets in this group.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing: {group_name}, Metric: {metric}, Type: {agg_type}\")\n",
    "            output_file = f\"table_{group_name}\"\n",
    "            create_ranking_table(dataset_list, output_file, metric, agg_type)\n",
    "\n",
    "print(\"\\n--- All ranking tables have been generated. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e04dc5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 7: Example: Display and Style a Result Table**\n",
    "\n",
    "This cell demonstrates how to load one of the generated tables and apply the custom styling to highlight the best ranks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load and display the 'average' AUC results for the general dataset group\n",
    "try:\n",
    "    df_example = pd.read_csv(r'..\\..\\tables\\average\\table_general_auc.csv', sep=';')\n",
    "    \n",
    "    # Separate the data from the summary rows for styling\n",
    "    df_data = df_example.iloc[:-2, :]\n",
    "    df_summary = df_example.iloc[-2:, :]\n",
    "    \n",
    "    # Apply styling to the ranking columns of the data part\n",
    "    # Note: This requires extracting ranks from the summary row first\n",
    "    ranking_summary_row = df_summary[df_summary['Dataset'] == 'Average Rank']\n",
    "    \n",
    "    # Display the styled table (will render correctly in a Jupyter environment)\n",
    "    # For simplicity, we display the raw summary here.\n",
    "    # A more complex function would be needed to style the final table with summary rows.\n",
    "    display(df_example)\n",
    "\n",
    "    # Example of styling just the ranks (without the summary)\n",
    "    # This requires creating the rank table separately first, which the main function does internally.\n",
    "    # For demonstration, we can re-calculate ranks here.\n",
    "    df_ranks_only = df_data.set_index('Dataset').rank(axis=1, method='min', ascending=True)\n",
    "    display(highlight_min_in_row(df_ranks_only))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Example file not found. Please run the main processing cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b6b5f",
   "metadata": {},
   "source": [
    "### **Cell 8: Prepare Data for Pair Plots**\n",
    "\n",
    "This cell processes the ranking tables generated previously to create summarized CSV files. These files contain the average ranks for different dataset groupings and are specifically formatted for generating pair plots in the next analysis step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247517a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration for Pair Plot Data Generation ---\n",
    "metrics = ['auc', 'adj_r_precision', 'adj_average_precision', 'adj_max_f1']\n",
    "aggregation_types = ['average', 'best']\n",
    "PAIRPLOT_OUTPUT_DIR = r'..\\..\\results\\experiments\\plot\\PAIRPLOT'\n",
    "\n",
    "# Define all dataset groupings using the dictionary from Cell 6\n",
    "# This ensures we use the same dataset lists as in the table generation step.\n",
    "dataset_groups = {\n",
    "    'general': all_dataset_names,\n",
    "    **get_datasets_by_feature_count(15, 25),\n",
    "    **get_datasets_by_instance_count(10000, 20000),\n",
    "    **get_datasets_by_categorical_percentage(34, 67),\n",
    "    **get_datasets_by_binary_percentage(50),\n",
    "    **get_datasets_by_context(),\n",
    "}\n",
    "\n",
    "def extract_average_rank(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Extracts the numerical average rank from the summary row of a ranking table.\"\"\"\n",
    "    # The 'Average Rank' is the last row\n",
    "    rank_row = df.iloc[-1].tolist()[1:]\n",
    "    return [float(str(rank).split('\\u00B1')[0]) for rank in rank_row]\n",
    "\n",
    "# --- Main Loop to Generate Pair Plot CSVs ---\n",
    "for agg_type in aggregation_types:\n",
    "    # Create the output directory for the current aggregation type\n",
    "    os.makedirs(os.path.join(PAIRPLOT_OUTPUT_DIR, agg_type), exist_ok=True)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"Generating pair plot data for: Type='{agg_type}', Metric='{metric}'\")\n",
    "\n",
    "        # --- 1. General Group ---\n",
    "        # Process all datasets together\n",
    "        df_general = create_ranking_table(dataset_groups['general'], 'table_general', metric=metric, agg_type=agg_type)\n",
    "        df_plot = pd.DataFrame({\n",
    "            'algorithm': df_general.columns.tolist()[1:],\n",
    "            'general': extract_average_rank(df_general),\n",
    "        })\n",
    "        df_plot.to_csv(os.path.join(PAIRPLOT_OUTPUT_DIR, agg_type, f'general_{metric}.csv'), sep=',', index=False)\n",
    "\n",
    "        # --- 2. Feature Count Group ---\n",
    "        df_low_feat = create_ranking_table(dataset_groups['low_feature'], 'table_feature_low', metric=metric, agg_type=agg_type)\n",
    "        df_med_feat = create_ranking_table(dataset_groups['medium_feature'], 'table_feature_medium', metric=metric, agg_type=agg_type)\n",
    "        df_high_feat = create_ranking_table(dataset_groups['high_feature'], 'table_feature_high', metric=metric, agg_type=agg_type)\n",
    "        df_plot = pd.DataFrame({\n",
    "            'algorithm': df_low_feat.columns.tolist()[1:],\n",
    "            'low': extract_average_rank(df_low_feat),\n",
    "            'medium': extract_average_rank(df_med_feat),\n",
    "            'high': extract_average_rank(df_high_feat)\n",
    "        })\n",
    "        df_plot.to_csv(os.path.join(PAIRPLOT_OUTPUT_DIR, agg_type, f'feature_{metric}.csv'), sep=',', index=False)\n",
    "\n",
    "        # --- 3. Instance Count Group ---\n",
    "        df_low_inst = create_ranking_table(dataset_groups['low_instance'], 'table_instance_low', metric=metric, agg_type=agg_type)\n",
    "        df_med_inst = create_ranking_table(dataset_groups['medium_instance'], 'table_instance_medium', metric=metric, agg_type=agg_type)\n",
    "        df_high_inst = create_ranking_table(dataset_groups['high_instance'], 'table_instance_high', metric=metric, agg_type=agg_type)\n",
    "        df_plot = pd.DataFrame({\n",
    "            'algorithm': df_low_inst.columns.tolist()[1:],\n",
    "            'low': extract_average_rank(df_low_inst),\n",
    "            'medium': extract_average_rank(df_med_inst),\n",
    "            'high': extract_average_rank(df_high_inst)\n",
    "        })\n",
    "        df_plot.to_csv(os.path.join(PAIRPLOT_OUTPUT_DIR, agg_type, f'instance_{metric}.csv'), sep=',', index=False)\n",
    "\n",
    "        # --- 4. Categorical Percentage Group (for mixed datasets) ---\n",
    "        df_low_cat = create_ranking_table(dataset_groups['low_categorical'], 'table_categorical_low', metric=metric, agg_type=agg_type)\n",
    "        df_med_cat = create_ranking_table(dataset_groups['medium_categorical'], 'table_categorical_medium', metric=metric, agg_type=agg_type)\n",
    "        df_high_cat = create_ranking_table(dataset_groups['high_categorical'], 'table_categorical_high', metric=metric, agg_type=agg_type)\n",
    "        df_plot = pd.DataFrame({\n",
    "            'algorithm': df_low_cat.columns.tolist()[1:],\n",
    "            'low': extract_average_rank(df_low_cat),\n",
    "            'medium': extract_average_rank(df_med_cat),\n",
    "            'high': extract_average_rank(df_high_cat)\n",
    "        })\n",
    "        df_plot.to_csv(os.path.join(PAIRPLOT_OUTPUT_DIR, agg_type, f'categorical_{metric}.csv'), sep=',', index=False)\n",
    "\n",
    "        # --- 5. Binary Percentage Group (for purely categorical datasets) ---\n",
    "        df_low_bin = create_ranking_table(dataset_groups['low_binary'], 'table_binary_low', metric=metric, agg_type=agg_type)\n",
    "        df_high_bin = create_ranking_table(dataset_groups['high_binary'], 'table_binary_high', metric=metric, agg_type=agg_type)\n",
    "        df_plot = pd.DataFrame({\n",
    "            'algorithm': df_low_bin.columns.tolist()[1:],\n",
    "            'low_binary': extract_average_rank(df_low_bin),\n",
    "            'high_binary': extract_average_rank(df_high_bin)\n",
    "        })\n",
    "        df_plot.to_csv(os.path.join(PAIRPLOT_OUTPUT_DIR, agg_type, f'binary_{metric}.csv'), sep=',', index=False)\n",
    "        \n",
    "        # --- 6. Context Group ---\n",
    "        df_medicine_bin = create_ranking_table(dataset_groups['medicine_context'], 'table_context_medicine', metric=metric, agg_type=agg_type)\n",
    "        df_finance_bin = create_ranking_table(dataset_groups['finance_context'], 'table_context_finance', metric=metric, agg_type=agg_type)\n",
    "        df_invasion_bin = create_ranking_table(dataset_groups['invasion_context'], 'table_context_invasion', metric=metric, agg_type=agg_type)\n",
    "        df_sciency_bin = create_ranking_table(dataset_groups['sciency_context'], 'table_context_sciency', metric=metric, agg_type=agg_type)\n",
    "        df_synthetic_bin = create_ranking_table(dataset_groups['synthetic_context'], 'table_context_synthetic', metric=metric, agg_type=agg_type)\n",
    "        df_plot = pd.DataFrame({\n",
    "            'algorithm': df_low_bin.columns.tolist()[1:],\n",
    "            'medicine_context': extract_average_rank(df_medicine_bin),\n",
    "            'finance_context': extract_average_rank(df_finance_bin),\n",
    "            'invasion_context': extract_average_rank(df_invasion_bin),\n",
    "            'sciency_context': extract_average_rank(df_sciency_bin),\n",
    "            'synthetic_context': extract_average_rank(df_synthetic_bin),\n",
    "        })\n",
    "        df_plot.to_csv(os.path.join(PAIRPLOT_OUTPUT_DIR, agg_type, f'context_{metric}.csv'), sep=',', index=False)\n",
    "\n",
    "\n",
    "print(\"\\n--- All data files for pair plots have been generated. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
