{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90341f61",
   "metadata": {},
   "source": [
    "### **Cell 1: Imports**\n",
    "\n",
    "This cell contains all the necessary library imports for the project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2bd0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from CriticalDifference import draw_cd_diagram\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ea10e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 2: Configuration of Result Files**\n",
    "\n",
    "This cell defines a dictionary mapping algorithm names to their corresponding result CSV files. This centralized configuration makes it easy to manage file paths.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    # CATEGORICAL ALGORITHMS\n",
    "    'AVF': r'..\\..\\results\\experiments\\algorithms\\AVF\\AVF.csv',\n",
    "    'CBRW': r'..\\..\\results\\experiments\\algorithms\\CBRW\\CBRW.csv',\n",
    "    'CompreX': r'..\\..\\results\\experiments\\algorithms\\CompreX\\COMPREX.csv',\n",
    "    'FPOF': r'..\\..\\results\\experiments\\algorithms\\FPOF\\FPOF.csv',\n",
    "    'POP': r'..\\..\\results\\experiments\\algorithms\\POP\\POP.csv',\n",
    "    'SCAN': r'..\\..\\results\\experiments\\algorithms\\SCAN\\SCAN.csv',\n",
    "    'SDRW': r'..\\..\\results\\experiments\\algorithms\\SDRW\\SDRW.csv',\n",
    "    'Zero++': r'..\\..\\results\\experiments\\algorithms\\Zero++\\ZERO++.csv',\n",
    "    \n",
    "    # NUMERICAL ALGORITHMS WITH DIFFERENT ENCODINGS\n",
    "    'LOF_ca': r'..\\..\\results\\experiments\\algorithms\\LOF\\ca\\CA.csv',\n",
    "    'KNN_ca': r'..\\..\\results\\experiments\\algorithms\\KNN\\ca\\CA.csv',\n",
    "    'iForest_ca': r'..\\..\\results\\experiments\\algorithms\\iForest\\ca\\CA.csv',\n",
    "    'FastABOD_ca': r'..\\..\\results\\experiments\\algorithms\\FastABOD\\ca\\CA.csv',\n",
    "    'DeepSVDD_ca': r'..\\..\\results\\experiments\\algorithms\\DeepSVDD\\ca\\CA.csv',\n",
    "    'McCatch_ca': r'..\\..\\results\\experiments\\algorithms\\McCatch\\ca\\CA.csv',\n",
    "    \n",
    "    'LOF_idf': r'..\\..\\results\\experiments\\algorithms\\LOF\\idf\\IDF.csv',\n",
    "    'KNN_idf': r'..\\..\\results\\experiments\\algorithms\\KNN\\idf\\IDF.csv',\n",
    "    'iForest_idf': r'..\\..\\results\\experiments\\algorithms\\iForest\\idf\\IDF.csv',\n",
    "    'FastABOD_idf': r'..\\..\\results\\experiments\\algorithms\\FastABOD\\idf\\IDF.csv',\n",
    "    'DeepSVDD_idf': r'..\\..\\results\\experiments\\algorithms\\DeepSVDD\\idf\\IDF.csv',\n",
    "    'McCatch_idf': r'..\\..\\results\\experiments\\algorithms\\McCatch\\idf\\IDF.csv',\n",
    "    \n",
    "    'LOF_onehot': r'..\\..\\results\\experiments\\algorithms\\LOF\\one_hot\\ONE_HOT.csv',\n",
    "    'KNN_onehot': r'..\\..\\results\\experiments\\algorithms\\KNN\\one_hot\\ONE_HOT.csv',\n",
    "    'iForest_onehot': r'..\\..\\results\\experiments\\algorithms\\iForest\\one_hot\\ONE_HOT.csv',\n",
    "    'FastABOD_onehot': r'..\\..\\results\\experiments\\algorithms\\FastABOD\\one_hot\\ONE_HOT.csv',\n",
    "    'DeepSVDD_onehot': r'..\\..\\results\\experiments\\algorithms\\DeepSVDD\\one_hot\\ONE_HOT.csv',\n",
    "    'McCatch_onehot': r'..\\..\\results\\experiments\\algorithms\\McCatch\\one_hot\\ONE_HOT.csv',\n",
    "    \n",
    "    'LOF_pivot': r'..\\..\\results\\experiments\\algorithms\\LOF\\pivot\\PIVOT.csv',\n",
    "    'KNN_pivot': r'..\\..\\results\\experiments\\algorithms\\KNN\\pivot\\PIVOT.csv',\n",
    "    'iForest_pivot': r'..\\..\\results\\experiments\\algorithms\\iForest\\pivot\\PIVOT.csv',\n",
    "    'FastABOD_pivot': r'..\\..\\results\\experiments\\algorithms\\FastABOD\\pivot\\PIVOT.csv',\n",
    "    'DeepSVDD_pivot': r'..\\..\\results\\experiments\\algorithms\\DeepSVDD\\pivot\\PIVOT.csv',\n",
    "    'McCatch_pivot': r'..\\..\\results\\experiments\\algorithms\\McCatch\\pivot\\PIVOT.csv',\n",
    "    \n",
    "    'LOF_nocat': r'..\\..\\results\\experiments\\algorithms\\LOF\\nocat\\NOCAT.csv',\n",
    "    'KNN_nocat': r'..\\..\\results\\experiments\\algorithms\\KNN\\nocat\\NOCAT.csv',\n",
    "    'iForest_nocat': r'..\\..\\results\\experiments\\algorithms\\iForest\\nocat\\NOCAT.csv',\n",
    "    'FastABOD_nocat': r'..\\..\\results\\experiments\\algorithms\\FastABOD\\nocat\\NOCAT.csv',\n",
    "    'DeepSVDD_nocat': r'..\\..\\results\\experiments\\algorithms\\DeepSVDD\\nocat\\NOCAT.csv',\n",
    "    'McCatch_nocat': r'..\\..\\results\\experiments\\algorithms\\McCatch\\nocat\\NOCAT.csv',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ced99",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 3: Load and Prepare Dataset Summary**\n",
    "\n",
    "This cell loads a summary of the datasets used in the experiments and prepares a sorted list of all unique dataset files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba2f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datasets_summary = pd.read_csv(r'..\\resume_datasets.csv', sep=';')\n",
    "all_datasets = sorted(df_datasets_summary['file'].unique().tolist())\n",
    "df_datasets_summary.sort_values(by=['%_categorics', '%_binaries'], inplace=True)\n",
    "df_datasets_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab674674",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 4: Verify Processed Files**\n",
    "\n",
    "This cell checks which datasets have been processed by each algorithm, creating a summary DataFrame to show the processing status with checkmarks (✔) or crosses (❌).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify processing status\n",
    "pd.set_option('display.max_columns', 100)\n",
    "processed_list = [all_datasets]\n",
    "column_names = ['Datasets']\n",
    "\n",
    "for name, path in results.items():\n",
    "    column_names.append(name)\n",
    "    status_list = []\n",
    "    try:\n",
    "        df_result = pd.read_csv(path, sep=';')\n",
    "        processed_datasets = sorted(df_result['dataset'].apply(lambda x: re.sub(r'_v[0-9]+', '', x).replace('.csv.csv', '.csv')).unique().tolist())\n",
    "        \n",
    "        for dataset_name in all_datasets:\n",
    "            if dataset_name in processed_datasets:\n",
    "                status_list.append('\\u2714')  # Checkmark\n",
    "            else:\n",
    "                status_list.append('\\u274C')  # Cross\n",
    "    except FileNotFoundError:\n",
    "        # If a result file doesn't exist, mark all as not processed\n",
    "        status_list = ['\\u274C'] * len(all_datasets)\n",
    "        \n",
    "    processed_list.append(status_list)\n",
    "    \n",
    "# Create a DataFrame from the collected data\n",
    "processed_dict = {col: data for col, data in zip(column_names, processed_list)}\n",
    "df_status = pd.DataFrame(processed_dict)\n",
    "df_status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff91c467",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 5: Define Evaluation Metrics**\n",
    "\n",
    "This cell lists the performance metrics that will be used for comparison throughout the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7316870",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = [\n",
    "    'auc',\n",
    "    'adj_r_precision',\n",
    "    'adj_average_precision',\n",
    "    'adj_max_f1'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c09c8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 6: Helper Functions for Data Aggregation**\n",
    "\n",
    "These functions are used to extract the best-performing row and calculate the average value for a given metric from a DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20649f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_with_highest_value(df, column):\n",
    "    \"\"\"Returns the row with the maximum value in the specified column.\"\"\"\n",
    "    max_value_index = df[column].idxmax()\n",
    "    return df.loc[max_value_index]\n",
    "\n",
    "def get_average_algorithm_value(df, column):\n",
    "    \"\"\"Calculates the mean value of a specified column.\"\"\"\n",
    "    mean_value = df[column].mean()\n",
    "    return mean_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1121a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 7: Result Processing and Analysis Functions**\n",
    "\n",
    "This block contains core functions for processing the experiment results, including averaging dataset versions, handling missing data, and preparing data for ranking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d282cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_dataset_versions(df):\n",
    "    \"\"\"\n",
    "    Calculates the average performance across different versions of the same dataset.\n",
    "    It groups by dataset name (ignoring version suffixes like _v01) and parameters.\n",
    "    \"\"\"\n",
    "    df.fillna('', inplace=True)\n",
    "    # Standardize dataset name by removing version suffixes (e.g., _v01)\n",
    "    df['dataset'] = df['dataset'].str.replace(r'_v[0-9]{2}', '', regex=True)\n",
    "    \n",
    "    rows = []\n",
    "    for dataset_name in df['dataset'].unique():\n",
    "        # Get unique parameter sets for the current dataset\n",
    "        parameters = df.query('dataset == @dataset_name')['parameter'].unique()\n",
    "        for param in parameters:\n",
    "            # Calculate the mean of metrics for this dataset and parameter set\n",
    "            mean_metrics = df.query('dataset == @dataset_name and parameter == @param')[evaluation_metrics].mean().tolist()\n",
    "            row = [dataset_name, param, df['algorithm'].unique()[0]] + mean_metrics\n",
    "            rows.append(row)\n",
    "            \n",
    "    df_averaged = pd.DataFrame(rows, columns=['dataset', 'parameter', 'algorithm'] + evaluation_metrics)\n",
    "    return df_averaged\n",
    "\n",
    "def add_missing_datasets(df, metric, diagram_type):\n",
    "    \"\"\"\n",
    "    Ensures that all algorithms have a result for every dataset.\n",
    "    If a result is missing, it's added with a default low-performance value.\n",
    "    \"\"\"\n",
    "    random_value = {'auc': 0.5, 'adj_r_precision': 0, 'adj_average_precision': 0, 'adj_max_f1': 0}\n",
    "    \n",
    "    # Determine column names, which can vary\n",
    "    dataset_col = 'dataset_name' if 'dataset_name' in df.columns else 'dataset'\n",
    "    algo_col = 'classifier_name' if 'classifier_name' in df.columns else 'algorithm'\n",
    "    \n",
    "    all_datasets_in_df = df[dataset_col].unique()\n",
    "    all_algorithms_in_df = df[algo_col].unique()\n",
    "    \n",
    "    rows_to_add = []\n",
    "    for d in all_datasets_in_df:\n",
    "        for a in all_algorithms_in_df:\n",
    "            if df[(df[dataset_col] == d) & (df[algo_col] == a)].empty:\n",
    "                # Assign a default value for the missing entry\n",
    "                new_row = {dataset_col: d, algo_col: a, 'value': random_value[metric], 'metric': metric, 'diagram': diagram_type}\n",
    "                rows_to_add.append(new_row)\n",
    "    \n",
    "    if rows_to_add:\n",
    "        df = pd.concat([df, pd.DataFrame(rows_to_add)], ignore_index=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def filter_datasets_by_list(df, dataset_list):\n",
    "    \"\"\"Filters a DataFrame to include only datasets from a specified list.\"\"\"\n",
    "    dataset_col = 'dataset_name' if 'dataset_name' in df.columns else 'dataset'\n",
    "    df[dataset_col] = df[dataset_col].str.replace(r'_v[0-9]{2}', '', regex=True)\n",
    "    return df[df[dataset_col].isin(dataset_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dbc935",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 8: Ranking and File Generation Functions**\n",
    "\n",
    "These functions are responsible for calculating ranks based on performance and saving them to CSV files for further analysis (e.g., with Wilcoxon signed-rank test).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a939ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ranks(values: list):\n",
    "    \"\"\"\n",
    "    Calculates ranks for a list of values, handling ties by averaging.\n",
    "    Example: [10, 20, 20, 30] -> [1, 2.5, 2.5, 4]\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to handle ranking easily\n",
    "    temp_df = pd.DataFrame({'values': values})\n",
    "    # 'average' method for ties is standard for statistical tests\n",
    "    temp_df['rank'] = temp_df['values'].rank(method='average', ascending=False)\n",
    "    return temp_df['rank'].tolist()\n",
    "\n",
    "def create_wilcoxon_rank_file(ranking_series, file_name):\n",
    "    \"\"\"Saves the average ranks of algorithms to a CSV file.\"\"\"\n",
    "    ranking_df = ranking_series.reset_index()\n",
    "    ranking_df.columns = ['algorithm', 'average_rank']\n",
    "    # The rank of the average_rank is not standard for Wilcoxon, but might be for other plots.\n",
    "    # Sticking to the original logic of re-ranking the average ranks.\n",
    "    ranking_df['rank_of_rank'] = calculate_ranks(ranking_df['average_rank'].tolist())\n",
    "    ranking_df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a2a2bb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 9: Calculate \"Best\" and \"Average\" Performance DataFrames**\n",
    "\n",
    "This block processes all result files to create two main DataFrames:\n",
    "1.  `df_bests`: Contains the best performance for each algorithm on each dataset (optimized parameters).\n",
    "2.  `df_averages`: Contains the average performance for each algorithm on each dataset across all its parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358735e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bests = []\n",
    "df_averages = []\n",
    "\n",
    "# BEST: Find the best parameter set for each algorithm on each dataset\n",
    "for metric in evaluation_metrics:\n",
    "    rows = []\n",
    "    for algo_name, file_path in results.items():\n",
    "        if os.path.isfile(file_path):\n",
    "            df = pd.read_csv(file_path, sep=';').drop_duplicates()\n",
    "            df_avg_versions = average_dataset_versions(df)\n",
    "            \n",
    "            for dataset_name in df_avg_versions['dataset'].unique():\n",
    "                # Get the best performing parameters for the current dataset\n",
    "                best_row = get_row_with_highest_value(\n",
    "                    df_avg_versions.query('dataset == @dataset_name'),\n",
    "                    metric\n",
    "                )\n",
    "                rows.append([best_row['dataset'], algo_name, best_row[metric], metric, 'best'])\n",
    "                \n",
    "    df_temp = filter_datasets_by_list(pd.DataFrame(rows, columns=['dataset', 'algorithm', 'value', 'metric', 'diagram']), all_datasets)\n",
    "    df_bests.append(df_temp)\n",
    "\n",
    "# AVERAGE: Calculate the average performance for each algorithm on each dataset\n",
    "for metric in evaluation_metrics:\n",
    "    rows = []\n",
    "    for algo_name, file_path in results.items():\n",
    "        if os.path.isfile(file_path):\n",
    "            df = pd.read_csv(file_path, sep=';').drop_duplicates()\n",
    "            df_avg_versions = average_dataset_versions(df)\n",
    "\n",
    "            for dataset_name in df_avg_versions['dataset'].unique():\n",
    "                # Get the average performance for the current dataset\n",
    "                mean_value = get_average_algorithm_value(\n",
    "                    df_avg_versions.query('dataset == @dataset_name'),\n",
    "                    metric\n",
    "                )\n",
    "                rows.append([dataset_name, algo_name, mean_value, metric, 'average'])\n",
    "                \n",
    "    df_temp = filter_datasets_by_list(pd.DataFrame(rows, columns=['dataset', 'algorithm', 'value', 'metric', 'diagram']), all_datasets)\n",
    "    df_averages.append(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb502d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 10: Generate Overall Critical Difference Diagrams**\n",
    "\n",
    "This cell generates and saves the main Critical Difference (CD) diagrams, comparing all algorithms across all datasets for both \"best\" and \"average\" performance scenarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories if they don't exist\n",
    "if not os.path.exists(r'..\\..\\results\\experiments\\plot'):\n",
    "    os.makedirs(r'..\\..\\results\\experiments\\plot', exist_ok=True)\n",
    "if not os.path.exists(r'..\\..\\results\\experiments\\tables\\best\\ranking'):\n",
    "    os.makedirs(r'..\\..\\results\\experiments\\tables\\best\\ranking', exist_ok=True)\n",
    "if not os.path.exists(r'..\\..\\results\\experiments\\tables\\average\\ranking'):\n",
    "    os.makedirs(r'..\\..\\results\\experiments\\tables\\average\\ranking', exist_ok=True)\n",
    "\n",
    "# Process BEST performance results\n",
    "for i, metric in enumerate(evaluation_metrics):\n",
    "    df_ = df_bests[i]\n",
    "    df_.rename(columns={'algorithm': 'classifier_name', 'dataset': 'dataset_name', 'value': 'accuracy'}, inplace=True)\n",
    "    df_ = add_missing_datasets(df_, metric, 'best')\n",
    "    df_bests[i] = df_\n",
    "    \n",
    "    output_path = f'..\\\\..\\\\results\\\\experiments\\\\plot\\\\best-{metric}'\n",
    "    try:\n",
    "        average_ranks = draw_cd_diagram(df_perf=df_.drop(columns=['metric', 'diagram']), title=f'Best Overall - Metric: {metric.replace(\"adj_\", \"\")}', labels=True, output=output_path)\n",
    "        create_wilcoxon_rank_file(average_ranks, f'..\\\\..\\\\results\\\\experiments\\\\tables\\\\best\\\\ranking\\\\best-{metric}.csv')\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating 'best' diagram for {metric}: {e}\")\n",
    "\n",
    "# Process AVERAGE performance results\n",
    "for i, metric in enumerate(evaluation_metrics):\n",
    "    df_ = df_averages[i]\n",
    "    df_.rename(columns={'algorithm': 'classifier_name', 'dataset': 'dataset_name', 'value': 'accuracy'}, inplace=True)\n",
    "    df_ = add_missing_datasets(df_, metric, 'average')\n",
    "    df_averages[i] = df_\n",
    "    \n",
    "    output_path = f'..\\\\..\\\\results\\\\experiments\\\\plot\\\\average-{metric}'\n",
    "    try:\n",
    "        average_ranks = draw_cd_diagram(df_perf=df_.drop(columns=['metric', 'diagram']), title=f'Average Overall - Metric: {metric.replace(\"adj_\", \"\")}', labels=True, output=output_path)\n",
    "        create_wilcoxon_rank_file(average_ranks, f'..\\\\..\\\\results\\\\experiments\\\\tables\\\\average\\\\ranking\\\\average-{metric}.csv')\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating 'average' diagram for {metric}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f9916",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 11: Dataset Grouping Functions**\n",
    "\n",
    "This section defines a series of functions to categorize datasets based on their properties, such as data type, number of features, number of instances, and domain context. These groupings are used to perform more detailed, comparative analyses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34078b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_by_type(summary_df):\n",
    "    \"\"\"Splits datasets into 'mixed' and 'categorical-only' types.\"\"\"\n",
    "    mixed = summary_df[summary_df['type'] == 'mix']['file'].tolist()\n",
    "    categorical = summary_df[summary_df['type'] == 'cat']['file'].tolist()\n",
    "    return mixed, categorical\n",
    "\n",
    "def get_datasets_by_num_features(summary_df, low, medium):\n",
    "    \"\"\"Groups datasets by the number of features.\"\"\"\n",
    "    low_list = summary_df[summary_df['features'] <= low]['file'].tolist()\n",
    "    medium_list = summary_df[(summary_df['features'] > low) & (summary_df['features'] <= medium)]['file'].tolist()\n",
    "    high_list = summary_df[summary_df['features'] > medium]['file'].tolist()\n",
    "    return low_list, medium_list, high_list\n",
    "\n",
    "def get_datasets_by_num_instances(summary_df, low, medium):\n",
    "    \"\"\"Groups datasets by the number of instances.\"\"\"\n",
    "    low_list = summary_df[summary_df['instances'] <= low]['file'].tolist()\n",
    "    medium_list = summary_df[(summary_df['instances'] > low) & (summary_df['instances'] <= medium)]['file'].tolist()\n",
    "    high_list = summary_df[summary_df['instances'] > medium]['file'].tolist()\n",
    "    return low_list, medium_list, high_list\n",
    "\n",
    "def get_datasets_by_percent_categorical(summary_df, low, medium):\n",
    "    \"\"\"Groups mixed-type datasets by the percentage of categorical features.\"\"\"\n",
    "    df_mixed = summary_df[summary_df['attr_categorics'] > 0]\n",
    "    low_list = df_mixed[df_mixed['%_categorics'] <= low]['file'].tolist()\n",
    "    medium_list = df_mixed[(df_mixed['%_categorics'] > low) & (df_mixed['%_categorics'] <= medium)]['file'].tolist()\n",
    "    high_list = df_mixed[df_mixed['%_categorics'] > medium]['file'].tolist()\n",
    "    return low_list, medium_list, high_list\n",
    "\n",
    "def get_datasets_by_binary_feature_threshold(summary_df, threshold):\n",
    "    \"\"\"\n",
    "    For purely categorical datasets, splits them based on the percentage of binary features.\n",
    "    \"\"\"\n",
    "    df_cat_only = summary_df[summary_df['%_categorics'] >= 100]\n",
    "    low_list = df_cat_only[df_cat_only['%_binaries'] <= threshold]['file'].tolist()\n",
    "    high_list = df_cat_only[df_cat_only['%_binaries'] > threshold]['file'].tolist()\n",
    "    return low_list, high_list\n",
    "\n",
    "def get_datasets_by_context(summary_df):\n",
    "    \"\"\"Splits datasets based on the presence of contextual features.\"\"\"\n",
    "    return ['thyroid_disease_variant1ori.csv', 'sick_sick_35_variant1ori.csv', 'cmc-nominal.csv', 'ecoli.csv', 'heart.csv', 'hepatitis.csv', 'lymphography.csv', 'nursery.csv'], ['bank-additional-ful-nominal_processed.csv', 'creditA_plus_42_variant1ori.csv', 'australian.csv', 'crx.csv', 'german.csv', 'Reuters-corn-100.csv'], ['kddcup99-corrected-u2rvsnormal-nominal-cleaned.csv', 'KDDTrain20R2LFS.csv', 'KDDTrain20ProbeFS.csv', 'KDDTrain20FS.csv'], ['solar-flare_FvsAll-cleaned_processed.csv', 'bands_band_16_variant1ori.csv', 'anneal.csv', 'covertype_nominal_4vs123567.csv', 'AID362red_train_allpossiblenominal.csv', 'list_attr_celeba_baldvsnonbald.csv', 'w7a-libsvm-nonsparse.csv'], ['scenario-1-1-rcat-0-icat.csv', 'scenario-1-2-rcat-0-icat.csv', 'scenario-1-3-rcat-0-icat.csv', 'scenario-1-4-rcat-0-icat.csv', 'scenario-1-5-rcat-0-icat.csv', 'scenario-1-6-rcat-0-icat.csv',\n",
    "                          'scenario-1-7-rcat-0-icat.csv', 'scenario-1-8-rcat-0-icat.csv', 'scenario-1-9-rcat-0-icat.csv', 'scenario-1-9-rcat-1-icat.csv', 'scenario-1-9-rcat-2-icat.csv', 'scenario-1-9-rcat-3-icat.csv',\n",
    "                          'scenario-1-9-rcat-4-icat.csv', 'scenario-1-9-rcat-5-icat.csv', 'scenario-1-9-rcat-6-icat.csv', 'scenario-1-9-rcat-7-icat.csv', 'scenario-1-9-rcat-8-icat.csv', 'scenario-1-9-rcat-9-icat.csv']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bbf8c6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 12: Generate Grouped Critical Difference Diagrams**\n",
    "\n",
    "This final, comprehensive cell uses the grouping functions to generate a wide array of CD diagrams. It creates comparisons for different data types, feature counts, instance counts, and categorical feature percentages. This allows for a nuanced understanding of how algorithms perform under various conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_grouped_diagrams(df_results, metric, dataset_group, group_name, title_prefix, output_folder):\n",
    "    \"\"\"Helper function to generate and save a CD diagram for a specific dataset group.\"\"\"\n",
    "    df_filtered = filter_datasets_by_list(df_results.copy(), dataset_group)\n",
    "    if df_filtered.empty or df_filtered['dataset_name'].nunique() < 2:\n",
    "        print(f\"Skipping '{group_name}' for metric '{metric}': Not enough data.\")\n",
    "        return\n",
    "\n",
    "    output_path = f'..\\\\..\\\\results\\\\experiments\\\\plot\\\\{output_folder}\\\\{title_prefix}-{group_name}-{metric}'\n",
    "    title = f'{title_prefix.replace(\"-\", \" \").title()} {metric} ({group_name})'\n",
    "    \n",
    "    try:\n",
    "        draw_cd_diagram(df_perf=df_filtered.drop(columns=['metric', 'diagram']), title=title, labels=True, output=output_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating diagram for {title}: {e}\")\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "for folder in ['feature', 'instance', 'type', 'percent-categorical', 'binary-feature', 'context']:\n",
    "    if not os.path.exists(f'..\\\\..\\\\results\\\\experiments\\\\{folder}'):\n",
    "        os.makedirs(f'..\\\\..\\\\results\\\\experiments\\\\plot\\\\{folder}', exist_ok=True)\n",
    "\n",
    "# --- Analysis by Data Type ---\n",
    "mixed_datasets, cat_datasets = get_datasets_by_type(df_datasets_summary)\n",
    "for i, metric in enumerate(evaluation_metrics):\n",
    "    generate_grouped_diagrams(df_bests[i], metric, mixed_datasets, 'Mix', 'best', 'type')\n",
    "    generate_grouped_diagrams(df_bests[i], metric, cat_datasets, 'Cat', 'best', 'type')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, mixed_datasets, 'Mix', 'average', 'type')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, cat_datasets, 'Cat', 'average', 'type')\n",
    "\n",
    "# --- Analysis by Number of Instances ---\n",
    "low_inst, med_inst, high_inst = get_datasets_by_num_instances(df_datasets_summary, 10000, 20000)\n",
    "for i, metric in enumerate(evaluation_metrics):\n",
    "    generate_grouped_diagrams(df_bests[i], metric, low_inst, 'Low-Instances', 'best', 'instance')\n",
    "    generate_grouped_diagrams(df_bests[i], metric, med_inst, 'Med-Instances', 'best', 'instance')\n",
    "    generate_grouped_diagrams(df_bests[i], metric, high_inst, 'High-Instances', 'best', 'instance')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, low_inst, 'Low-Instances', 'average', 'instance')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, med_inst, 'Med-Instances', 'average', 'instance')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, high_inst, 'High-Instances', 'average', 'instance')\n",
    "\n",
    "# --- Analysis by Number of Features ---\n",
    "low_feat, med_feat, high_feat = get_datasets_by_num_features(df_datasets_summary, 15, 25)\n",
    "for i, metric in enumerate(evaluation_metrics):\n",
    "    generate_grouped_diagrams(df_bests[i], metric, low_feat, 'Low-Features', 'best', 'feature')\n",
    "    generate_grouped_diagrams(df_bests[i], metric, med_feat, 'Med-Features', 'best', 'feature')\n",
    "    generate_grouped_diagrams(df_bests[i], metric, high_feat, 'High-Features', 'best', 'feature')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, low_feat, 'Low-Features', 'average', 'feature')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, med_feat, 'Med-Features', 'average', 'feature')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, high_feat, 'High-Features', 'average', 'feature')\n",
    "\n",
    "# --- Analysis by Percentage of Categorical Features (for mixed datasets) ---\n",
    "low_cat, med_cat, high_cat = get_datasets_by_percent_categorical(df_datasets_summary, 34, 67)\n",
    "for i, metric in enumerate(evaluation_metrics):\n",
    "    generate_grouped_diagrams(df_bests[i], metric, low_cat, 'Low-Categorical', 'best', 'percent-categorical')\n",
    "    generate_grouped_diagrams(df_bests[i], metric, med_cat, 'Med-Categorical', 'best', 'percent-categorical')\n",
    "    generate_grouped_diagrams(df_bests[i], metric, high_cat, 'High-Categorical', 'best', 'percent-categorical')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, low_cat, 'Low-Categorical', 'average', 'percent-categorical')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, med_cat, 'Med-Categorical', 'average', 'percent-categorical')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, high_cat, 'High-Categorical', 'average', 'percent-categorical')\n",
    "\n",
    "# --- Analysis by Percentage of Binary Features (for 100% categorical datasets) ---\n",
    "low_bin, high_bin = get_datasets_by_binary_feature_threshold(df_datasets_summary, 50)\n",
    "for i, metric in enumerate(evaluation_metrics):\n",
    "    generate_grouped_diagrams(df_bests[i], metric, low_bin, 'Low-Binary', 'best', 'binary-feature')\n",
    "    generate_grouped_diagrams(df_bests[i], metric, high_bin, 'High-Binary', 'best', 'binary-feature')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, low_bin, 'Low-Binary', 'average', 'binary-feature')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, high_bin, 'High-Binary', 'average', 'binary-feature')\n",
    "    \n",
    "# --- Analysis by Context of Dataset ---\n",
    "medicine_context, finance_context, invasion_context, sciency_context, synthetic_context = get_datasets_by_context(df_datasets_summary)\n",
    "for i, metric in enumerate(evaluation_metrics):\n",
    "    generate_grouped_diagrams(df_bests[i], metric, medicine_context, 'Medicine', 'best', 'context')\n",
    "    generate_grouped_diagrams(df_bests[i], metric, finance_context, 'Finance', 'best', 'context')\n",
    "    generate_grouped_diagrams(df_bests[i], metric, invasion_context, 'Invasion', 'best', 'context')\n",
    "    generate_grouped_diagrams(df_bests[i], metric, sciency_context, 'Sciency', 'best', 'context')\n",
    "    generate_grouped_diagrams(df_bests[i], metric, synthetic_context, 'Synthetic', 'best', 'context')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, medicine_context, 'Medicine', 'average', 'context')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, finance_context, 'Finance', 'average', 'context')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, invasion_context, 'Invasion', 'average', 'context')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, sciency_context, 'Sciency', 'average', 'context')\n",
    "    generate_grouped_diagrams(df_averages[i], metric, synthetic_context, 'Synthetic', 'average', 'context')\n",
    "\n",
    "print(\"\\n--- All processing and diagram generation complete. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
