{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f0a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook preprocesses raw datasets (.arff and .csv) by performing the following steps:\n",
    "# 1. Loading the data and a configuration file for data types.\n",
    "# 2. Applying correct data types to columns.\n",
    "# 3. Standardizing the target column (named 'outlier') to 'yes' and 'no' labels.\n",
    "# 4. Normalizing numerical features to a [0, 1] scale.\n",
    "# 5. Removing duplicate rows.\n",
    "# 6. Downsampling the majority class ('outliers') to a maximum of 5% of the data.\n",
    "# 7. Generating 10 versions of each processed dataset.\n",
    "# 8. Saving the processed dataframes to a 'processed' directory.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from scipy.io import arff\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize_numerical_features(df: pd.DataFrame, num_cols: pd.DataFrame, cat_cols: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes the numerical columns of a DataFrame to a [0, 1] scale using MinMaxScaler.\n",
    "    Some datasets have categorical columns with numbers, but those should not be normalized.\n",
    "    \n",
    "    Args:\n",
    "        df: The original DataFrame to preserve column order.\n",
    "        num_cols: DataFrame containing only the numerical columns.\n",
    "        cat_cols: DataFrame containing only the categorical columns.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame with numerical columns scaled and original column order maintained.\n",
    "    \"\"\"\n",
    "    if not num_cols.empty:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        \n",
    "        # Scale numerical data, creating a new DataFrame with original index and column names\n",
    "        scaled_values = min_max_scaler.fit_transform(num_cols)\n",
    "        df_scaled_num = pd.DataFrame(scaled_values, columns=num_cols.columns, index=num_cols.index)\n",
    "\n",
    "        # Concatenate scaled numerical columns with original categorical columns\n",
    "        df_normalized = pd.concat([df_scaled_num, cat_cols], axis=1)\n",
    "\n",
    "        # Ensure the column order is the same as the original DataFrame\n",
    "        return df_normalized[df.columns]\n",
    "    else:\n",
    "        # Return the original DataFrame if there are no numerical columns\n",
    "        return df\n",
    "\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes duplicate rows from the DataFrame.\n",
    "    \"\"\"\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "\n",
    "def remove_rows_with_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes rows with missing values, assuming '?' represents a missing value.\n",
    "    Note: This function is defined but not used in the main processing pipeline.\n",
    "    \"\"\"\n",
    "    return df.replace('?', np.nan).dropna()\n",
    "\n",
    "\n",
    "def downsample_outliers(file: str, df: pd.DataFrame, percent_limit: int, versions: int) -> list:\n",
    "    \"\"\"\n",
    "    Reduces the number of outliers to a specified percentage by random removal.\n",
    "    Generates multiple versions of the downsampled dataset.\n",
    "\n",
    "    Args:\n",
    "        file: The name of the dataset file (for logging).\n",
    "        df: The input DataFrame, which must contain an 'outlier' column.\n",
    "        percent_limit: The maximum allowed percentage of outliers.\n",
    "        versions: The number of downsampled DataFrame versions to create.\n",
    "\n",
    "    Returns:\n",
    "        A list of DataFrames, each being a downsampled version.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    for i in range(versions):\n",
    "        num_rows = len(df)\n",
    "        outlier_indices = list(df[df['outlier'] == 'yes'].index)\n",
    "        num_outliers = len(outlier_indices)\n",
    "        df_copy = df.copy(deep=True)\n",
    "\n",
    "        # Check if the percentage of outliers exceeds the limit\n",
    "        if num_outliers > num_rows * (percent_limit / 100):\n",
    "            target_outlier_count = int(num_rows * (percent_limit / 100))\n",
    "            \n",
    "            # Number of outliers to remove\n",
    "            num_to_remove = num_outliers - target_outlier_count\n",
    "            \n",
    "            # Randomly select outliers to remove\n",
    "            indices_to_remove = random.sample(outlier_indices, num_to_remove)\n",
    "            \n",
    "            df_copy.drop(indices_to_remove, inplace=True)\n",
    "            df_copy.reset_index(drop=True, inplace=True)\n",
    "            dataframes.append(df_copy)\n",
    "        else:\n",
    "            print(f'{file}: Already has {percent_limit}% or fewer outliers.')\n",
    "            dataframes.append(df_copy)\n",
    "            # If the condition is met, no need to generate more identical versions\n",
    "            return dataframes\n",
    "        \n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def save_processed_dataframes(base_path: str, original_filename: str, dataframes: list):\n",
    "    \"\"\"\n",
    "    Saves a list of dataframes to CSV files in a 'processed' subdirectory.\n",
    "\n",
    "    Args:\n",
    "        base_path: The root directory where the 'processed' folder should be.\n",
    "        original_filename: The name of the original file, used for naming the new files.\n",
    "        dataframes: A list of DataFrames to save.\n",
    "    \"\"\"\n",
    "    processed_dir = os.path.join(base_path, 'processed')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    # Sanitize filename by removing original extension if present\n",
    "    file_stem = original_filename.split('.')[0]\n",
    "\n",
    "    for i, df in enumerate(dataframes, 1):\n",
    "        # Format filename as '..._v01.csv', '..._v02.csv', etc.\n",
    "        output_filename = f\"{file_stem}_v{i:02d}.csv\"\n",
    "        output_path = os.path.join(processed_dir, output_filename)\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "def split_numerical_categorical(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into two DataFrames: one with numerical columns and one with categorical/object columns.\n",
    "    \"\"\"\n",
    "    num_cols = df.select_dtypes(include=np.number)\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category'])\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "\n",
    "def apply_custom_dtypes(df: pd.DataFrame, dataset_name: str, config: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies data types to DataFrame columns based on a configuration dictionary.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to modify.\n",
    "        dataset_name: The name of the dataset (without extension) to look up in the config.\n",
    "        config: The dictionary containing dtype information.\n",
    "\n",
    "    Returns:\n",
    "        The DataFrame with updated dtypes.\n",
    "    \"\"\"\n",
    "    dataset_key = dataset_name.split('.')[0]\n",
    "    if dataset_key in config:\n",
    "        dtypes = config[dataset_key]\n",
    "        columns = df.columns\n",
    "        # Handle wildcard '...' to apply one type to all columns\n",
    "        if '...' in dtypes:\n",
    "            dtype = dtypes[0]\n",
    "            return df.astype(dict.fromkeys(columns, dtype))\n",
    "        else:\n",
    "            return df.astype(dict(zip(columns, dtypes)))\n",
    "    print(f'Notice: Dtype configuration not found for {dataset_name}. Using inferred types.')\n",
    "    return df\n",
    "\n",
    "\n",
    "def standardize_outlier_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes the 'outlier' column labels.\n",
    "    It assumes the minority class is the outlier ('yes') and the majority class is the nominal ('no').\n",
    "    \"\"\"\n",
    "    if 'outlier' not in df.columns:\n",
    "        return df\n",
    "        \n",
    "    # Count the frequency of each value in the 'outlier' column\n",
    "    value_counts = df['outlier'].value_counts()\n",
    "    \n",
    "    # Identify the most and least frequent values\n",
    "    if len(value_counts) == 2:\n",
    "        majority_val = value_counts.idxmax()\n",
    "        minority_val = value_counts.idxmin()\n",
    "        \n",
    "        # Replace values to standardize them\n",
    "        df['outlier'] = df['outlier'].replace({\n",
    "            majority_val: 'no',\n",
    "            minority_val: 'yes'\n",
    "        })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dedbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Processing Pipeline ---\n",
    "\n",
    "# Define the path to the datasets\n",
    "# It's recommended to use relative paths or environment variables for better portability.\n",
    "DATASET_PATH = r'..\\..\\datasets\\exp'\n",
    "CONFIG_PATH = r'..\\config_dataset.json'\n",
    "\n",
    "# Load dataset type configuration\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        dataset_dtypes_config = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Configuration file not found at {CONFIG_PATH}\")\n",
    "    dataset_dtypes_config = {}\n",
    "\n",
    "# Get list of context\n",
    "for context in os.listdir(DATASET_PATH):\n",
    "    context_path = os.path.join(DATASET_PATH, context)\n",
    "\n",
    "    if not os.path.isdir(context_path):\n",
    "        continue\n",
    "    \n",
    "    # Get the list of datasets to process\n",
    "    try:\n",
    "        datasets = os.listdir(context_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Dataset directory not found at {DATASET_PATH}\")\n",
    "        datasets = []\n",
    "\n",
    "    # Process each dataset\n",
    "    for dataset_filename in datasets:\n",
    "        print(f\"Processing: {dataset_filename}\")\n",
    "        file_path = os.path.join(context_path, dataset_filename)\n",
    "        \n",
    "        # Load dataset based on file extension\n",
    "        if dataset_filename.endswith('.arff'):\n",
    "            data, meta = arff.loadarff(file_path)\n",
    "            df = pd.DataFrame(data)\n",
    "            # The last column is assumed to be the target/outlier column\n",
    "            target_col_name = meta.names()[-1]\n",
    "            \n",
    "            # Decode byte strings, a common requirement for .arff files\n",
    "            str_cols = df.select_dtypes(include=[object]).columns\n",
    "            df[str_cols] = df[str_cols].apply(lambda x: x.str.decode('utf-8'))\n",
    "            \n",
    "            df.rename(columns={target_col_name: 'outlier'}, inplace=True)\n",
    "            \n",
    "        elif dataset_filename.endswith('.csv'):\n",
    "            # Use 'python' engine for flexibility with separators\n",
    "            df = pd.read_csv(file_path, engine='python')\n",
    "        else:\n",
    "            # Skip files that are not .arff or .csv\n",
    "            print(f\"Skipping unsupported file type: {dataset_filename}\")\n",
    "            continue\n",
    "\n",
    "        # --- Preprocessing Steps ---\n",
    "        \n",
    "        # 1. Drop 'id' column if it exists\n",
    "        if 'id' in df.columns:\n",
    "            df.drop(columns=['id'], inplace=True)\n",
    "            \n",
    "        # 2. Apply predefined data types from config file\n",
    "        df = apply_custom_dtypes(df, dataset_filename, dataset_dtypes_config)\n",
    "        \n",
    "        # 3. Standardize outlier labels to 'yes' (minority) and 'no' (majority)\n",
    "        df = standardize_outlier_labels(df)\n",
    "        \n",
    "        # 4. Split into numerical and categorical columns for normalization\n",
    "        num_cols, cat_cols = split_numerical_categorical(df)\n",
    "        \n",
    "        # 5. Normalize numerical features\n",
    "        df = normalize_numerical_features(df, num_cols, cat_cols)\n",
    "        \n",
    "        # 6. Remove duplicate records\n",
    "        df = remove_duplicates(df)\n",
    "        \n",
    "        # 7. Downsample outliers to a max of 5%, creating 10 versions\n",
    "        df.rename(columns={df.columns[-1]: 'outlier'}, inplace=True)\n",
    "        processed_versions = downsample_outliers(dataset_filename, df, percent_limit=5, versions=10)\n",
    "        \n",
    "        # 8. Save the processed dataframes\n",
    "        save_processed_dataframes(context_path + '/processed', dataset_filename, processed_versions)\n",
    "\n",
    "print(\"\\nProcessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d762c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv(r'C:\\Users\\pipip\\Downloads\\CategoricalDatasets-main\\CategoricalDatasets\\files\\datasets\\exp\\finance\\crx.csv', engine='python')\n",
    "df_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
