{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6e25d2",
   "metadata": {},
   "source": [
    "\n",
    "### **Cell 1: Imports and Initial Setup**\n",
    "\n",
    "This cell imports the necessary libraries and installs `kaleido` if needed, which is required for exporting plots to static image files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832123a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kaleido in c:\\users\\pipip\\appdata\\roaming\\python\\python313\\site-packages (1.1.0)\n",
      "Requirement already satisfied: choreographer>=1.0.10 in c:\\users\\pipip\\appdata\\roaming\\python\\python313\\site-packages (from kaleido) (1.2.0)\n",
      "Requirement already satisfied: logistro>=1.0.8 in c:\\users\\pipip\\appdata\\roaming\\python\\python313\\site-packages (from kaleido) (2.0.1)\n",
      "Requirement already satisfied: orjson>=3.10.15 in c:\\users\\pipip\\appdata\\roaming\\python\\python313\\site-packages (from kaleido) (3.11.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\pipip\\appdata\\roaming\\python\\python313\\site-packages (from kaleido) (24.2)\n",
      "Requirement already satisfied: pytest-timeout>=2.4.0 in c:\\users\\pipip\\appdata\\roaming\\python\\python313\\site-packages (from kaleido) (2.4.0)\n",
      "Requirement already satisfied: simplejson>=3.19.3 in c:\\users\\pipip\\appdata\\roaming\\python\\python313\\site-packages (from choreographer>=1.0.10->kaleido) (3.20.2)\n",
      "Requirement already satisfied: pytest>=7.0.0 in c:\\users\\pipip\\appdata\\roaming\\python\\python313\\site-packages (from pytest-timeout>=2.4.0->kaleido) (8.4.2)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\pipip\\appdata\\roaming\\python\\python313\\site-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (0.4.6)\n",
      "Requirement already satisfied: iniconfig>=1 in c:\\users\\pipip\\appdata\\roaming\\python\\python313\\site-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.3.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\pipip\\appdata\\roaming\\python\\python313\\site-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\users\\pipip\\appdata\\roaming\\python\\python313\\site-packages (from pytest>=7.0.0->pytest-timeout>=2.4.0->kaleido) (2.19.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Kaleido is needed for exporting plotly figures to static images.\n",
    "%pip install -U kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacae523",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 2: Configuration**\n",
    "\n",
    "This cell centralizes all configurations, including paths, algorithm groups, and metric names, making the script easier to manage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e0b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path for algorithm results\n",
    "BASE_RESULTS_PATH = r'..\\..\\..\\results\\base_experiments\\algorithms'\n",
    "\n",
    "# Dictionary mapping conversion methods to their respective algorithms and result file paths\n",
    "ALGORITHMS_BY_METHOD = {\n",
    "    'ca': {\n",
    "        'DeepSVDD_ca': r'DeepSVDD\\ca\\CA.csv',\n",
    "        'FastABOD_ca': r'FastABOD\\ca\\CA.csv',\n",
    "        'iForest_ca': r'iForest\\ca\\CA.csv',\n",
    "        'KNN_ca': r'KNN\\ca\\CA.csv',\n",
    "        'LOF_ca': r'LOF\\ca\\CA.csv',\n",
    "        'McCatch_ca': r'McCatch\\ca\\CA.csv',\n",
    "    },\n",
    "    'idf': {\n",
    "        'DeepSVDD_idf': r'DeepSVDD\\idf\\IDF.csv',\n",
    "        'FastABOD_idf': r'FastABOD\\idf\\IDF.csv',\n",
    "        'iForest_idf': r'iForest\\idf\\IDF.csv',\n",
    "        'KNN_idf': r'KNN\\idf\\IDF.csv',\n",
    "        'LOF_idf': r'LOF\\idf\\IDF.csv',\n",
    "        'McCatch_idf': r'McCatch\\idf\\IDF.csv',\n",
    "    },\n",
    "    'onehot': {\n",
    "        'DeepSVDD_one': r'DeepSVDD\\one_hot\\ONE_HOT.csv',\n",
    "        'FastABOD_one': r'FastABOD\\one_hot\\ONE_HOT.csv',\n",
    "        'iForest_one': r'iForest\\one_hot\\ONE_HOT.csv',\n",
    "        'KNN_one': r'KNN\\one_hot\\ONE_HOT.csv',\n",
    "        'LOF_one': r'LOF\\one_hot\\ONE_HOT.csv',\n",
    "        'McCatch_one': r'McCatch\\one_hot\\ONE_HOT.csv',\n",
    "    },\n",
    "    'pivot': {\n",
    "        'DeepSVDD_pivot': r'DeepSVDD\\pivot\\PIVOT.csv',\n",
    "        'FastABOD_pivot': r'FastABOD\\pivot\\PIVOT.csv',\n",
    "        'iForest_pivot': r'iForest\\pivot\\PIVOT.csv',\n",
    "        'KNN_pivot': r'KNN\\pivot\\PIVOT.csv',\n",
    "        'LOF_pivot': r'LOF\\pivot\\PIVOT.csv',\n",
    "        'McCatch_pivot': r'McCatch\\pivot\\PIVOT.csv',\n",
    "    },\n",
    "    'nocat': {\n",
    "        'DeepSVDD_nocat': r'DeepSVDD\\nocat\\NOCAT.csv',\n",
    "        'FastABOD_nocat': r'FastABOD\\nocat\\NOCAT.csv',\n",
    "        'iForest_nocat': r'iForest\\nocat\\NOCAT.csv',\n",
    "        'KNN_nocat': r'KNN\\nocat\\NOCAT.csv',\n",
    "        'LOF_nocat': r'LOF\\nocat\\NOCAT.csv',\n",
    "        'McCatch_nocat': r'McCatch\\nocat\\NOCAT.csv',\n",
    "    }\n",
    "}\n",
    "\n",
    "# List of metrics to be analyzed\n",
    "METRICS = ['auc', 'adj_r_precision', 'adj_average_precision', 'adj_max_f1']\n",
    "\n",
    "# Mapping from metric names to display names for plots\n",
    "METRIC_DISPLAY_NAMES = {\n",
    "    'auc': 'AUC',\n",
    "    'adj_r_precision': 'P@n',\n",
    "    'adj_average_precision': 'AP',\n",
    "    'adj_max_f1': 'Max-F1'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c5e1c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 3: Load Dataset Metadata**\n",
    "\n",
    "This cell loads the summary file containing metadata for all datasets to create a master list of datasets to be included in the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054203b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the summary file containing metadata for all datasets\n",
    "df_summary = pd.read_csv(r'..\\..\\resume_datasets.csv', sep=';')\n",
    "\n",
    "# Get a sorted, unique list of all dataset file names\n",
    "ALL_DATASET_NAMES = sorted(df_summary['file'].str.replace('.csv', '').unique().tolist())\n",
    "print(f\"Loaded metadata for {len(ALL_DATASET_NAMES)} datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d17fe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 4: Data Processing Helper Functions**\n",
    "\n",
    "This cell contains reusable functions for cleaning and preparing the performance data before plotting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_dataset_versions(df, metrics_to_average):\n",
    "    \"\"\"\n",
    "    Averages results for datasets that have multiple versions (e.g., _v01, _v02).\n",
    "    It standardizes the dataset name and then groups by dataset and parameter to average the scores.\n",
    "    \"\"\"\n",
    "    if 'dataset' not in df.columns:\n",
    "        return df\n",
    "        \n",
    "    # Standardize dataset names by removing version suffixes (e.g., _v01)\n",
    "    df['dataset'] = df['dataset'].str.replace(r'_v[0-9]{1,2}', '', regex=True)\n",
    "    \n",
    "    # Define columns to group by\n",
    "    grouping_cols = ['dataset', 'parameter', 'algorithm']\n",
    "    \n",
    "    # Calculate the mean for the specified metric columns\n",
    "    df_averaged = df.groupby(grouping_cols)[metrics_to_average].mean().reset_index()\n",
    "    \n",
    "    return df_averaged\n",
    "\n",
    "def fill_missing_scores(series, metric_name):\n",
    "    \"\"\"Fills NaN values in a series with a default score based on the metric.\"\"\"\n",
    "    default_scores = {\n",
    "        'auc': 0.5,\n",
    "        'adj_r_precision': 0.0,\n",
    "        'adj_average_precision': 0.0,\n",
    "        'adj_max_f1': 0.0\n",
    "    }\n",
    "    return series.fillna(default_scores.get(metric_name, 0.0))\n",
    "\n",
    "def filter_by_dataset_list(df, dataset_list):\n",
    "    \"\"\"Filters a DataFrame to keep only the datasets present in the provided list.\"\"\"\n",
    "    # Standardize dataset names before filtering\n",
    "    df['dataset'] = df['dataset'].str.replace(r'(_v[0-9]{1,2})?\\.csv$', '', regex=True)\n",
    "    return df[df['dataset'].isin(dataset_list)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14355bc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 5: Plotting Function**\n",
    "\n",
    "This cell defines the main function responsible for creating, styling, and saving the boxplots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a263b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_boxplot(dataframe, value_col, category_col, metric_name, title='Boxplot', save=True):\n",
    "    \"\"\"\n",
    "    Generates and displays a boxplot using Plotly, with options to save the figure.\n",
    "    It also removes outliers based on the IQR method before plotting.\n",
    "    \"\"\"\n",
    "    # Define a consistent color map for the conversion methods\n",
    "    color_map = {\n",
    "         \"ca\": \"#636EFA\",\n",
    "         \"idf\": \"#00CC96\",\n",
    "         \"onehot\": \"#FFA15A\",\n",
    "         \"pivot\": \"#AB63FA\",\n",
    "         \"nocat\": \"#EF553B\"\n",
    "    }\n",
    "    \n",
    "    # --- Remove outliers using the IQR method ---\n",
    "    # This step is done to improve the readability of the boxplot visualization.\n",
    "    Q1 = dataframe[value_col].quantile(0.25)\n",
    "    Q3 = dataframe[value_col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_filtered = dataframe[(dataframe[value_col] >= lower_bound) & (dataframe[value_col] <= upper_bound)]\n",
    "    \n",
    "    # Create the boxplot figure\n",
    "    fig = px.box(\n",
    "        df_filtered, \n",
    "        x=category_col, \n",
    "        y=value_col, \n",
    "        title=title, \n",
    "        color=\"method\", \n",
    "        color_discrete_map=color_map\n",
    "    )\n",
    "    \n",
    "    # Update layout for a cleaner, more professional look\n",
    "    fig.update_layout(\n",
    "        boxmode='group',\n",
    "        title=None,\n",
    "        xaxis_title='Conversion Methods',\n",
    "        yaxis_title=metric_name,\n",
    "        showlegend=False,\n",
    "        font=dict(size=14),\n",
    "        xaxis=dict(title_font=dict(size=16)),\n",
    "        yaxis=dict(title_font=dict(size=16))\n",
    "    )\n",
    "    fig.update_traces(width=0.6) # Adjust box width\n",
    "    \n",
    "    # --- Add Median Annotations ---\n",
    "    # Calculate medians for each category to display on the plot\n",
    "    medians = df_filtered.groupby('method')[value_col].median().round(4)\n",
    "    for category, median_val in medians.items():\n",
    "        fig.add_annotation(\n",
    "            x=category,\n",
    "            y=median_val,\n",
    "            text=f\"{median_val}\",\n",
    "            showarrow=False,\n",
    "            yshift=10, # Shift text slightly above the median line\n",
    "            font=dict(color='black', size=12)\n",
    "        )\n",
    "        \n",
    "    fig.show()\n",
    "    \n",
    "    # Save the figure if requested\n",
    "    if save:\n",
    "        output_dir = r'..\\..\\..\\results\\base_experiments\\plot\\BOXPLOT'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, f\"boxplot_{metric_name.lower()}.png\")\n",
    "        fig.write_image(output_path)\n",
    "        print(f\"Saved plot to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a1be5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 6: Main Processing and Plot Generation**\n",
    "\n",
    "This is the main execution block. It iterates through each metric, loads the corresponding data, processes it using the helper functions, and generates a boxplot comparing the different conversion methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell processes the results for the 'average' performance case.\n",
    "# It calculates the average performance of each conversion method across all its algorithms for each dataset.\n",
    "\n",
    "for metric in METRICS:\n",
    "    print(f\"--- Processing metric: {metric} ---\")\n",
    "    \n",
    "    all_methods_data = []\n",
    "    \n",
    "    # Iterate over each conversion method (ca, idf, etc.)\n",
    "    for method, algorithms in ALGORITHMS_BY_METHOD.items():\n",
    "        \n",
    "        all_algos_in_method = []\n",
    "        \n",
    "        # Load data for each algorithm within the current method\n",
    "        for algo_name, file_path in algorithms.items():\n",
    "            full_path = os.path.join(BASE_RESULTS_PATH, file_path)\n",
    "            if os.path.exists(full_path):\n",
    "                try:\n",
    "                    df_algo = pd.read_csv(full_path, sep=';')\n",
    "                    # Standardize dataset names and filter to keep only relevant ones\n",
    "                    df_algo = filter_by_dataset_list(df_algo, ALL_DATASET_NAMES)\n",
    "                    # Average results for datasets with multiple versions\n",
    "                    df_algo = average_dataset_versions(df_algo, [metric])\n",
    "                    all_algos_in_method.append(df_algo)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not process file {full_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"File not found, skipping: {full_path}\")\n",
    "\n",
    "        # If no data was loaded for this method, skip to the next one\n",
    "        if not all_algos_in_method:\n",
    "            continue\n",
    "            \n",
    "        # Combine data from all algorithms in the current method\n",
    "        df_method = pd.concat(all_algos_in_method, ignore_index=True)\n",
    "        \n",
    "        # Calculate the average score for the method on each dataset\n",
    "        # This averages the performance of all algorithms (DeepSVDD, iForest, etc.) for that method\n",
    "        df_method_avg = df_method.groupby('dataset')[metric].mean().reset_index()\n",
    "        \n",
    "        # Add a column to identify the method\n",
    "        df_method_avg['method'] = method\n",
    "        \n",
    "        all_methods_data.append(df_method_avg)\n",
    "\n",
    "    # Combine the averaged data from all methods into a final DataFrame\n",
    "    if not all_methods_data:\n",
    "        print(f\"No data found for metric {metric}. Skipping plot generation.\")\n",
    "        continue\n",
    "        \n",
    "    df_final_for_plot = pd.concat(all_methods_data, ignore_index=True)\n",
    "    \n",
    "    # Rename the metric column to a generic 'value' for the plotting function\n",
    "    df_final_for_plot.rename(columns={metric: 'value'}, inplace=True)\n",
    "    \n",
    "    # Fill any missing values with the appropriate default for the metric\n",
    "    df_final_for_plot['value'] = fill_missing_scores(df_final_for_plot['value'], metric)\n",
    "    \n",
    "    # Generate the boxplot for the current metric\n",
    "    display_name = METRIC_DISPLAY_NAMES.get(metric, metric)\n",
    "    generate_boxplot(\n",
    "        df_final_for_plot, \n",
    "        value_col='value', \n",
    "        category_col='method', \n",
    "        metric_name=display_name, \n",
    "        title=f'Conversion Method Performance ({display_name})'\n",
    "    )\n",
    "\n",
    "print(\"\\n--- All plots generated. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
