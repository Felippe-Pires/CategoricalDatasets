{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9ca210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from Pivot_Based import Relation\n",
    "from prince import MCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c80795",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 2: Encoding and Data Transformation Functions**\n",
    "\n",
    "This cell centralizes all the helper functions used for data transformation, such as encoding categorical variables and performing correspondence analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correspondence_analysis(data, n_components=3):\n",
    "    \"\"\"\n",
    "    Perform Multiple Correspondence Analysis (MCA) on the given categorical data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): A DataFrame containing categorical data.\n",
    "        n_components (int): The number of dimensions to reduce to.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed data with reduced dimensions.\n",
    "    \"\"\"\n",
    "    mca = MCA(\n",
    "        n_components=n_components,\n",
    "        n_iter=3,\n",
    "        copy=True,\n",
    "        check_input=True,\n",
    "        engine='sklearn',\n",
    "        random_state=42\n",
    "    )\n",
    "    return mca.fit_transform(data)\n",
    "    \n",
    "\n",
    "def apply_idf_encoding(df, attribute):\n",
    "    \"\"\"\n",
    "    Calculates and applies the Inverse Document Frequency (IDF) to a specific attribute.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        attribute (str): The name of the column to apply IDF encoding to.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new column containing the IDF values.\n",
    "    \"\"\"\n",
    "    N = len(df)\n",
    "    if N == 0:\n",
    "        return df\n",
    "\n",
    "    # Calculate frequency of each value\n",
    "    value_counts = df[attribute].value_counts()\n",
    "\n",
    "    # Calculate ln(N / frequency) for each unique value\n",
    "    idf_values = {value: math.log(N / freq) for value, freq in value_counts.items()}\n",
    "\n",
    "    # Create a new column with the calculated IDF values\n",
    "    df[attribute + '_IDF'] = df[attribute].map(idf_values)\n",
    "\n",
    "    return df\n",
    "\n",
    "def apply_one_hot_encoding(df, attribute):\n",
    "    \"\"\"\n",
    "    Applies One-Hot Encoding to a specified column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        attribute (str): The name of the column to encode.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the specified column one-hot encoded.\n",
    "    \"\"\"\n",
    "    df_encoded = pd.get_dummies(df, columns=[attribute], dtype=int)\n",
    "    return df_encoded\n",
    "\n",
    "def apply_nocat(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses a DataFrame by:\n",
    "    1. Removing categorical columns that are not binary.\n",
    "    2. Encoding binary categorical columns to 0 and 1.\n",
    "    3. Normalizing numerical columns (with more than 2 unique values) to a [0, 1] scale.\n",
    "    4. Columns named 'outlier' or 'class' are ignored and left unchanged.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    cols_to_drop = []\n",
    "    numeric_cols_to_normalize = []\n",
    "    binary_cat_cols_to_encode = []\n",
    "    \n",
    "    for col in processed_df.columns:\n",
    "        # Skip processing for 'outlier' and 'class' columns (case-insensitive)\n",
    "        if col.lower() in ['outlier', 'class']:\n",
    "            continue\n",
    "\n",
    "        # Check if the column is of a numeric type\n",
    "        if pd.api.types.is_numeric_dtype(processed_df[col]):\n",
    "            # If it's numeric and has more than 2 unique values, it should be normalized\n",
    "            if processed_df[col].nunique() > 2:\n",
    "                numeric_cols_to_normalize.append(col)\n",
    "        else:\n",
    "            # Handle non-numeric (categorical) columns\n",
    "            if processed_df[col].nunique() == 2:\n",
    "                binary_cat_cols_to_encode.append(col)\n",
    "            elif processed_df[col].nunique() > 2:\n",
    "                cols_to_drop.append(col)\n",
    "                \n",
    "    # Drop the identified non-binary categorical columns\n",
    "    processed_df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"Dropped columns: {cols_to_drop}\")\n",
    "\n",
    "    # Encode binary categorical columns to 0/1\n",
    "    if binary_cat_cols_to_encode:\n",
    "        for col in binary_cat_cols_to_encode:\n",
    "            processed_df[col] = processed_df[col].astype('category').cat.codes\n",
    "        print(f\"Encoded binary columns: {binary_cat_cols_to_encode}\")\n",
    "    \n",
    "    # Normalize the identified numeric columns\n",
    "    if numeric_cols_to_normalize:\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        processed_df[numeric_cols_to_normalize] = scaler.fit_transform(processed_df[numeric_cols_to_normalize])\n",
    "        print(f\"Normalized columns: {numeric_cols_to_normalize}\")\n",
    "        \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a7c5d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 3: Data Loading and Preprocessing Functions**\n",
    "\n",
    "This cell contains functions for loading data, detecting delimiters, applying data types, normalizing features, and standardizing labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delimiter(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Detects the delimiter of a CSV file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        delimiter = csv.Sniffer().sniff(csvfile.read(1024)).delimiter\n",
    "    return delimiter\n",
    "\n",
    "def convert_int_to_str(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts all integer columns to string type to treat them as categorical.\n",
    "    \"\"\"\n",
    "    for col in df.select_dtypes(include='int').columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    return df\n",
    "\n",
    "def split_numerical_categorical(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into numerical and categorical columns.\n",
    "    \"\"\"\n",
    "    num_cols = df.select_dtypes(include=np.number)\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category'])\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def apply_custom_dtypes(df: pd.DataFrame, dataset_name: str, config: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies data types to DataFrame columns based on a configuration dictionary.\n",
    "    \"\"\"\n",
    "    # Remove versioning from dataset name (e.g., _v01) to match config key\n",
    "    dataset_key = re.sub(r'_v[0-9]+', '', dataset_name.split('.')[0])\n",
    "    \n",
    "    if dataset_key in config:\n",
    "        dtypes = config[dataset_key]\n",
    "        columns = df.columns\n",
    "        if '...' in dtypes:\n",
    "            # Apply a single dtype to all columns\n",
    "            dtype = dtypes[0]\n",
    "            return df.astype(dict.fromkeys(columns, dtype))\n",
    "        else:\n",
    "            # Apply specific dtypes to corresponding columns\n",
    "            return df.astype(dict(zip(columns, dtypes)))\n",
    "            \n",
    "    print(f'Notice: Dtype configuration not found for {dataset_key}. Using inferred types.')\n",
    "    return df\n",
    "\n",
    "def normalize_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes all columns except the last one (target) to a [0, 1] scale.\n",
    "    \"\"\"\n",
    "    features = df.drop(columns=['outlier'])\n",
    "    target = df['outlier']\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    scaled_features = min_max_scaler.fit_transform(features)\n",
    "    \n",
    "    df_scaled = pd.DataFrame(scaled_features, columns=features.columns, index=df.index)\n",
    "    df_scaled[target.name] = target\n",
    "    return df_scaled\n",
    "    \n",
    "def standardize_outlier_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes the last column to 'outlier' and its labels to 'yes' (minority) and 'no' (majority).\n",
    "    \"\"\"\n",
    "    outlier_col_name = df.columns[-1]\n",
    "    df = df.rename(columns={outlier_col_name: 'outlier'})\n",
    "    \n",
    "    value_counts = df['outlier'].value_counts()\n",
    "    \n",
    "    if len(value_counts) == 2:\n",
    "        majority_val = value_counts.idxmax()\n",
    "        minority_val = value_counts.idxmin()\n",
    "        \n",
    "        df['outlier'] = df['outlier'].replace({\n",
    "            majority_val: 'no',\n",
    "            minority_val: 'yes'\n",
    "        })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc947b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Cell 4: Main Processing Pipeline**\n",
    "\n",
    "This is the main execution block. It iterates through the specified datasets, applies the preprocessing and encoding steps, and saves the transformed files into structured directories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d95bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# List of dataset directories to process.\n",
    "# Add the relative paths to your dataset folders here.\n",
    "DATASET_PATHS = r'..\\..\\datasets\\experiments'\n",
    "CONFIG_FILE = r'..\\config_dataset.json'\n",
    "\n",
    "# --- Load Configuration ---\n",
    "try:\n",
    "    with open(CONFIG_FILE, 'r') as f:\n",
    "        dataset_dtypes_config = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Configuration file not found at {CONFIG_FILE}\")\n",
    "    dataset_dtypes_config = {}\n",
    "\n",
    "# --- Main Loop ---\n",
    "for context_path in os.listdir(DATASET_PATHS):\n",
    "    dataset_dir = os.path.join(DATASET_PATHS, context_path, 'processed')\n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n--- Processing directory: {context_path} ---\")\n",
    "        \n",
    "    # Create output directories\n",
    "    output_base = os.path.join(dataset_dir, 'number')\n",
    "    os.makedirs(os.path.join(output_base, 'one_hot'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_base, 'idf'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_base, 'ca'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_base, 'pivot'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_base, 'nocat'), exist_ok=True)\n",
    "\n",
    "    # Process each file in the directory\n",
    "    for filename in os.listdir(dataset_dir):\n",
    "        \n",
    "        if os.path.isdir(os.path.join(dataset_dir, filename)):\n",
    "            continue\n",
    "    \n",
    "        file_path = os.path.join(dataset_dir, filename)\n",
    "        \n",
    "        if os.path.isdir(file_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_csv(file_path, engine='python')\n",
    "\n",
    "        # --- Preprocessing Steps ---\n",
    "        df = apply_custom_dtypes(df, filename, dataset_dtypes_config)\n",
    "        df = convert_int_to_str(df)\n",
    "        df = standardize_outlier_label(df)\n",
    "\n",
    "        # --- Encoding and Saving ---\n",
    "        \n",
    "        # 1. Pivot-Based\n",
    "        relation = Relation.read_csv(file_path, len(df.columns)-1, True)\n",
    "        relation.normalize()\n",
    "        relation.save(os.path.join(output_base, 'pivot'))\n",
    "\n",
    "        # 2. One-Hot Encoding\n",
    "        df_one_hot = df.copy()\n",
    "        for col in df_one_hot.columns:\n",
    "            if df_one_hot[col].dtype == 'object' and col != 'outlier':\n",
    "                df_one_hot = apply_one_hot_encoding(df_one_hot, col)\n",
    "        df_one_hot = normalize_features(df_one_hot)\n",
    "        df_one_hot.to_csv(os.path.join(output_base, 'one_hot', filename), index=False)\n",
    "\n",
    "        # 3. IDF Encoding\n",
    "        df_idf = df.copy()\n",
    "        for col in df_idf.columns:\n",
    "            if df_idf[col].dtype == 'object' and col != 'outlier':\n",
    "                df_idf = apply_idf_encoding(df_idf, col)\n",
    "                df_idf.drop(columns=[col], inplace=True)\n",
    "        df_idf = normalize_features(df_idf)\n",
    "        df_idf.to_csv(os.path.join(output_base, 'idf', filename), index=False)\n",
    "\n",
    "        # 4. Correspondence Analysis (CA)\n",
    "        features_df = df.drop(columns=['outlier'])\n",
    "        num_features, cat_features = split_numerical_categorical(features_df)\n",
    "        if not cat_features.empty:\n",
    "            ca_features = calculate_correspondence_analysis(cat_features, n_components=len(cat_features.columns))\n",
    "            ca_features.columns = [f'Cat_{i+1}' for i in range(ca_features.shape[1])]\n",
    "            df_ca = pd.concat([num_features.reset_index(drop=True), ca_features.reset_index(drop=True)], axis=1)\n",
    "        else:\n",
    "            df_ca = num_features\n",
    "            \n",
    "        df_ca['outlier'] = df['outlier']\n",
    "        df_ca = normalize_features(df_ca)\n",
    "        df_ca.to_csv(os.path.join(output_base, 'ca', filename), index=False)\n",
    "        \n",
    "        # 5. Non Categorical Feature (NOCAT)\n",
    "        df_nocat = df.copy()\n",
    "        df_nocat = apply_nocat(df_nocat)\n",
    "        df_nocat.to_csv(os.path.join(output_base, 'nocat', filename), index=False)\n",
    "\n",
    "print(\"\\n--- All processing complete. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
